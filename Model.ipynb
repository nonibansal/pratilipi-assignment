{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9db3637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import svds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c12d357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Content Based Filtering \n",
    "class Recommendation:\n",
    "    def __init__(self, content_data, data):\n",
    "        \n",
    "        data = data.loc[:, [\"user_id\", \"pratilipi_id\", \"read_percent\", \"updated_at\"]]\n",
    "        data = data.rename(columns={\"updated_at\": \"date_read\"})\n",
    "        data = (data.assign(date_read=lambda x: pd.to_datetime(x[\"date_read\"]))\n",
    "                    .sort_values(by=[\"date_read\"])\n",
    "                    .reset_index(drop=True))\n",
    "\n",
    "        mark_75 = data.shape[0]*0.75\n",
    "        self.train = data.loc[0:mark_75-1].reset_index(drop=True)\n",
    "        self.test = data.loc[mark_75:].reset_index(drop=True)\n",
    "        self.content_data = content_data\n",
    "        del data\n",
    "        self.preprcoess_user_data()\n",
    "        self.preprocess_content_data()\n",
    "        \n",
    "    def normalize(self, pred_ratings):\n",
    "        return (pred_ratings - pred_ratings.min()) / (pred_ratings.max() - pred_ratings.min())\n",
    "    \n",
    "    def cosine_sim(self, v1, v2):\n",
    "        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "    \n",
    "    def one_hot_enconde(sefl, df, enc_col):\n",
    "        ohe_df = pd.get_dummies(df[enc_col])\n",
    "        ohe_df.reset_index(drop = True, inplace = True)\n",
    "        return pd.concat([df, ohe_df], axis = 1)\n",
    "    \n",
    "    def preprcoess_user_data(self):\n",
    "        train = self.train.groupby([\"user_id\", \"pratilipi_id\"]).agg({\"read_percent\": \"mean\"}).reset_index()\n",
    "        interesting_users = train.groupby(\"user_id\")[\"pratilipi_id\"].count().reset_index(name=\"count\")\n",
    "        interesting_users = interesting_users.loc[interesting_users[\"count\"] >= 20, \"user_id\"].tolist()\n",
    "        interesting_pratilipi = train.groupby(\"pratilipi_id\")[\"user_id\"].count().reset_index(name=\"count\")\n",
    "        interesting_pratilipi = interesting_pratilipi.loc[interesting_pratilipi[\"count\"] >= 20, \n",
    "                                                          \"pratilipi_id\"].tolist()\n",
    "        train = train.loc[(train[\"user_id\"].isin(interesting_users)) & \n",
    "                          (train[\"pratilipi_id\"].isin(interesting_pratilipi))].reset_index(drop=True)\n",
    "        \n",
    "        pratilipi_ids = list(train[\"pratilipi_id\"].unique())\n",
    "        user_ids = list(train[\"user_id\"].unique())\n",
    "        user_ids_dict = {}\n",
    "        pratilipi_ids_dict = {}\n",
    "        k = 0\n",
    "        for user_id in user_ids:\n",
    "            user_ids_dict[user_id] = k\n",
    "            k += 1\n",
    "        k = 0\n",
    "        for pratilipi_id in pratilipi_ids:\n",
    "            pratilipi_ids_dict[pratilipi_id] = k\n",
    "            k += 1\n",
    "            \n",
    "        data_matrix = np.zeros((len(user_ids), len(pratilipi_ids))).astype(np.float16)\n",
    "        for i in range(train.shape[0]):\n",
    "            user_id_ix = user_ids_dict.get(train.loc[i, \"user_id\"])\n",
    "            pratilipi_id_ix = pratilipi_ids_dict.get(train.loc[i, \"pratilipi_id\"])\n",
    "            read_percent = train.loc[i, \"read_percent\"]\n",
    "\n",
    "            data_matrix[user_id_ix, pratilipi_id_ix] = read_percent\n",
    "        del train\n",
    "        \n",
    "        data_matrix = csr_matrix(data_matrix)\n",
    "        u, s, v = svds(data_matrix, k=100)\n",
    "        s = np.diag(s)\n",
    "        pred_ratings = np.dot(np.dot(u, s), v)\n",
    "        pred_ratings = self.normalize(pred_ratings)\n",
    "        pred_ratings = pd.DataFrame(\n",
    "            pred_ratings,\n",
    "            columns = pratilipi_ids,\n",
    "            index = user_ids\n",
    "        ).transpose()\n",
    "        del u\n",
    "        del s\n",
    "        del v\n",
    "        del data_matrix\n",
    "        self.user_data_recommendation = pred_ratings\n",
    "        \n",
    "    def preprocess_content_data(self):\n",
    "        \n",
    "        self.content_data[\"category_name_present\"] = 0\n",
    "        self.content_data.loc[self.content_data[\"category_name\"].isna() == False, \"category_name_present\"] = 1\n",
    "\n",
    "        self.content_data = (pd.pivot(self.content_data, index=[\"author_id\", \"pratilipi_id\", \"reading_time\", \n",
    "                                                                \"updated_at\", \"published_at\"], \n",
    "                                      columns=\"category_name\", values=\"category_name_present\")\n",
    "                               .reset_index().fillna(0).rename_axis(None, axis=1))\n",
    "        self.content_data = (self.content_data.assign(published_at=lambda x: pd.to_datetime(x[\"published_at\"]))\n",
    "                                              .assign(published_year=lambda x: pd.to_datetime(x[\"published_at\"]).dt.year)\n",
    "                                              .assign(author_id=lambda x: x[\"author_id\"].astype(str).fillna(\"author_nan\"))\n",
    "                                              .assign(published_year=lambda x: x[\"published_year\"].astype(str).fillna(\"published_year_nan\")))\n",
    "        \n",
    "        self.content_data = self.content_data.drop(columns = [\"published_at\", \"updated_at\"])\n",
    "        \n",
    "        label_encoder_author = LabelEncoder()\n",
    "        label_encoder_author.fit(self.content_data[\"author_id\"])\n",
    "        self.content_data = self.content_data.assign(author_id=lambda x: label_encoder_author.transform(x[\"author_id\"]))\n",
    "        ohe_df = pd.get_dummies(self.content_data[\"published_year\"])\n",
    "        ohe_df.reset_index(drop = True, inplace = True)\n",
    "        self.content_data = pd.concat([self.content_data, ohe_df], axis = 1)        \n",
    "        \n",
    "        #Features Extracted from User Data\n",
    "        pratilipi_reads = self.train.groupby(\"pratilipi_id\").size().reset_index(name=\"total_reads\")\n",
    "        pratilipi_percent_read = (self.train.groupby(\"pratilipi_id\")\n",
    "                                            .agg({\"read_percent\": \"mean\"})\n",
    "                                            .reset_index())\n",
    "        pratilipi_unique_reads = (self.train.groupby(\"pratilipi_id\")\n",
    "                                            .agg({\"user_id\": \"nunique\"})\n",
    "                                            .reset_index()\n",
    "                                            .rename(columns={\"user_id\": \"unique_reads\"}))\n",
    "        pratilipi_50_unique_reads = (self.train.loc[self.train[\"read_percent\"] >= 50.0, :]\n",
    "                                               .reset_index(drop=True))\n",
    "        pratilipi_50_unique_reads = (pratilipi_50_unique_reads.groupby(\"pratilipi_id\")\n",
    "                                                              .agg({\"user_id\": \"nunique\"})\n",
    "                                                              .reset_index()\n",
    "                                                              .rename(columns={\"user_id\": \"unique_50_reads\"}))\n",
    "        pratilipi_50_reads = (self.train.loc[self.train[\"read_percent\"] >= 50.0, :]\n",
    "                                        .reset_index(drop=True))\n",
    "        pratilipi_50_reads = (pratilipi_50_unique_reads.groupby(\"pratilipi_id\")\n",
    "                                                       .size()\n",
    "                                                       .reset_index(name=\"total_50_reads\"))\n",
    "        self.content_data = (self.content_data.merge(pratilipi_reads, on=\"pratilipi_id\", how=\"left\")\n",
    "                                              .fillna(0))\n",
    "        self.content_data = (self.content_data.merge(pratilipi_percent_read, on=\"pratilipi_id\", how=\"left\")\n",
    "                                              .fillna(0))\n",
    "        self.content_data = (self.content_data.merge(pratilipi_unique_reads, on=\"pratilipi_id\", how=\"left\")\n",
    "                                              .fillna(0))\n",
    "        self.content_data = (self.content_data.merge(pratilipi_50_unique_reads, on=\"pratilipi_id\", how=\"left\")\n",
    "                                              .fillna(0))\n",
    "        self.content_data = (self.content_data.merge(pratilipi_50_reads, on=\"pratilipi_id\", how=\"left\")\n",
    "                                              .fillna(0))\n",
    "        self.content_data = self.content_data.drop([\"published_year\"], axis=1)\n",
    "        \n",
    "        min_max_columns = [\"reading_time\", \"total_reads\", \"read_percent\", \"unique_reads\", \n",
    "                           \"unique_50_reads\", \"total_50_reads\"]\n",
    "        min_max_scale = MinMaxScaler()\n",
    "        min_max_scale.fit(self.content_data[min_max_columns])\n",
    "        self.content_data[min_max_columns] = min_max_scale.transform(self.content_data[min_max_columns])\n",
    "        \n",
    "        self.content_data_recommendation = self.content_data.set_index(\"pratilipi_id\")\n",
    "        \n",
    "    \n",
    "    def get_top_n_pratilip_from_content(self, pratilipi_id, n_rec=5):\n",
    "        try:\n",
    "            inputVec = self.content_data_recommendation.loc[pratilipi_id].values\n",
    "            self.content_data_recommendation[\"sim\"]= self.content_data_recommendation.apply(\n",
    "                lambda x: self.cosine_sim(inputVec, x.values), axis=1)\n",
    "            output = self.content_data_recommendation.nlargest(columns=\"sim\", n=n_rec)\n",
    "            return output.reset_index().loc[:, [\"pratilipi_id\", \"sim\"]]\n",
    "        except:\n",
    "            return pd.DataFrame(columns=[\"pratilipi_id\", \"sim\"])\n",
    "    \n",
    "    def get_top_n_pratilip_from_userid(self, user_id, n_rec=5):\n",
    "        try:\n",
    "            usr_pred = (self.user_data_recommendation[user_id]\n",
    "                            .sort_values(ascending = False)\n",
    "                            .reset_index()\n",
    "                            .rename(columns = {user_id : \"sim\"}))\n",
    "            usr_pred = (usr_pred.sort_values(by = \"sim\", ascending = False).head(n_recs)\n",
    "                                .rename(columns={\"index\": \"pratilipi_id\"}))\n",
    "            return usr_pred\n",
    "        except:\n",
    "            return pd.DataFrame(columns=[\"pratilipi_id\", \"sim\"])\n",
    "                    \n",
    "    def predict(self):\n",
    "        output = {}\n",
    "        test_userids = list(self.test[\"user_id\"].unique())\n",
    "        for user_id in test_userids:\n",
    "            dummy = self.get_top_n_pratilip_from_userid(user_id, 5)\n",
    "            already_read = []\n",
    "            for pratilipi_id in list(self.train.loc[self.test[\"user_id\"] == user_id, \"pratilipi_id\"].unique()):\n",
    "                    dummy = pd.concat([dummy, self.get_top_n_pratilip_from_content(pratilipi_id, 5)])\n",
    "                    already_read.append(pratilipi_id)\n",
    "            dummy = dummy.sort_values(by=\"sim\", ascending=False).reset_index(drop=True)\n",
    "            dummy = dummy.loc[dummy[\"pratilipi_id\"].notin(already_read)].reset_index(drop=True)\n",
    "            output[user_id] = dummy.loc[0:4, \"pratilipi_id\"].tolist()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2dc29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation = Recommendation(pd.read_csv(\"metadata.csv\"), pd.read_csv(\"user-interactions.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0871cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06efbfe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
